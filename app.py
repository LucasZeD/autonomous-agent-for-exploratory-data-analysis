# /ui/app.py
import sys
import os
import streamlit as st
import pandas as pd
import base64
import re
from llm.llm_factory import LLMFactory
from graph.eda_graph import create_eda_graph
from tools.rag_tool import setup_vectorstore
from langchain_core.messages import AIMessage, HumanMessage
from PIL import UnidentifiedImageError

st.set_page_config(page_title="Agente de An치lise de CSV", layout="wide")

st.title("Agente para An치lise de Dados (E.D.A.)")
st.markdown("Desenvolvido com `LangGraph` para an치lises de alta acur치cia.")

# --- Inicializa칞칚o do Estado da Sess칚o ---
def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "graph_runner" not in st.session_state:
        st.session_state.graph_runner = None

init_session_state()

# --- BARRA LATERAL (CONFIGURA칂칏ES) ---
with st.sidebar:
    st.header("丘뙖잺 Configura칞칫es")
    
    uploaded_csv = st.file_uploader("1. Fa칞a o upload do seu CSV", type="csv")
    
    uploaded_pdfs = st.file_uploader(
        "2. (Opcional) Adicione PDFs de contexto (RAG)",
        type="pdf",
        accept_multiple_files=True,
        help="Forne칞a documentos PDF que contenham informa칞칫es sobre o seu CSV. O agente usar치 esses arquivos para dar respostas mais ricas e contextuais."
    )
    
    llm_provider = st.selectbox("3. Escolha o modelo de IA", ("Gemini", "GPT", "LocalLM"))
    
    api_key = None
    if llm_provider in ["GPT", "Gemini"]:
        api_key = st.text_input(f"Insira a chave de API do {llm_provider}", type="password", help="N칚o 칠 obrigat칩rio o uso de chave para o Gemini, o desenvolvedor j치 forneceu uma.")

    if st.button("游 Iniciar Agente"):
        if uploaded_csv is not None:
            with st.spinner("Processando arquivos e construindo o grafo..."):
                try:
                    df = pd.read_csv(uploaded_csv)
                    
                    if uploaded_pdfs:
                        setup_vectorstore(uploaded_pdfs)
                        st.success("Base de conhecimento (RAG) criada com sucesso!")
                    
                    llm = LLMFactory.create_llm(llm_provider, api_key)
                    st.session_state.graph_runner = create_eda_graph(llm, df)
                    
                    st.session_state.messages = []
                    st.success(f"Agente inicializado com {llm_provider}. Pronto para an치lise!")
                except Exception as e:
                    st.error(f"Erro na inicializa칞칚o: {e}")
        else:
            st.warning("Por favor, carregue um arquivo CSV para come칞ar.")

# --- 츼REA PRINCIPAL DO CHAT ---
def display_chat_history():
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            # Expander para detalhes de racioc칤nio
            if "details" in msg:
                with st.expander("Ver Racioc칤nio do Agente."):
                    st.markdown("##### Plano de An치lise")
                    st.markdown(msg["details"]["plan"], unsafe_allow_html=True)

                    st.markdown("##### C칩digo Executado")
                    st.code(msg["details"]["code"], language="python")
                    
                    st.markdown("##### Resultado Bruto")
                    # Remo칞칚o plot do gr치fico
                    raw_result = re.sub(r'\[PLOT_DATA:.*?\]', '[Visualiza칞칚o gerada com sucesso]', msg["details"]["result"])
                    # st.text(raw_result)
                    st.code(raw_result, language="text")
            # Resposta do modelo
            st.markdown(msg["content"])
            
            # Gr치fico gerado
            if "image" in msg and msg["image"]:
                try:
                    st.image(base64.b64decode(msg["image"]))
                except Exception as e:
                    st.warning(f"Falha ao renderizar imagem: {e}")


if st.session_state.graph_runner:
    st.info(f"Agente pronto! Fa칞a perguntas sobre o arquivo `{uploaded_csv.name}`.")
    display_chat_history()
    
    if prompt := st.chat_input("Fa칞a uma pergunta sobre seus dados..."):
        st.session_state.messages.append({"role": "user", "content": prompt, "lc_message": HumanMessage(content=prompt)})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("O agente est치 executando o workflow de an치lise..."):
                # chat_history = [msg["lc_message"] for msg in st.session_state.messages[:-1]]

                # final_state = st.session_state.graph_runner(prompt, chat_history)

                # conclusion = final_state.get("conclusion", "N칚o foi poss칤vel gerar uma conclus칚o.")

                # result_text = str(final_state.get("execution_result", ""))

                final_state = st.session_state.graph_runner(prompt, st.session_state.get("messages", []))

                conclusion = final_state.get("conclusion", "N칚o foi poss칤vel gerar uma conclus칚o.")

                execution_details = {
                    "plan": final_state.get("plan", "Plano n칚o dispon칤vel."),
                    "code": final_state.get("code_to_execute", "C칩digo n칚o dispon칤vel."),
                    "result": str(final_state.get("execution_result", "Resultado n칚o dispon칤vel."))
                }

                result_text = execution_details["result"]
                plot_match = re.search(r'\[PLOT_DATA:(.*?)\]', result_text)

                # assistant_message = {"role": "assistant", "content": conclusion, "lc_message": AIMessage(content=conclusion)}
                assistant_message = {"role": "assistant", "content": conclusion, "details": execution_details}
                
                # if plot_match:
                #     img_data = plot_match.group(1)
                #     st.markdown(conclusion)
                #     st.image(base64.b64decode(img_data))
                #     assistant_message["image"] = img_data
                # else:
                #     st.markdown(conclusion)
                st.markdown(conclusion)
                if plot_match:
                    img_data = plot_match.group(1).strip()
                    assistant_message["image"] = img_data
                    # Tenta renderizar a imagem, tratando poss칤veis erros
                    try:
                        # Verifica se img_data n칚o est치 vazio antes de tentar decodificar
                        if img_data:
                            st.image(base64.b64decode(img_data))
                        else:
                            st.warning("O agente indicou a gera칞칚o de um gr치fico, mas os dados da imagem est칚o ausentes.")
                    except (base64.binascii.Error, UnidentifiedImageError):
                        st.warning("N칚o foi poss칤vel renderizar a visualiza칞칚o. O agente pode ter retornado um resultado textual em vez de um gr치fico.")
                
                st.session_state.messages.append(assistant_message)
                st.rerun()
else:
    st.markdown("### Bem-vindo ao Agente de An치lise de Dados!")
    st.markdown("Esta ferramenta permite que voc칡 converse com seus dados em arquivos CSV para extrair insights de forma r치pida e intuitiva.")
    st.markdown("#### Como Come칞ar:")
    st.markdown("1.  **Abra a barra lateral** no canto superior esquerdo (칤cone `>>>`).")
    st.markdown("2.  **Fa칞a o upload** do seu arquivo CSV.")
    st.markdown("3.  **Escolha o modelo** de IA que deseja usar.")
    st.markdown("4.  **Insira a chave API** da IA que deseja usar.")
    st.markdown("5.  Clique em **'Iniciar Agente'** e comece a fazer perguntas!")